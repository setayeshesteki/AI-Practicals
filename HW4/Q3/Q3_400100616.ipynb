{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department <br>\n",
    "FALL 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "Practical Assignment 4 - Neural Networks <br>\n",
    "<font color=696880 size=4>\n",
    "Arash Ziyaei Razban - Mahan Bayhaghi\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Data\n",
    "Please fill in your details below to help us keep track of your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: `Setayesh Esteki`\n",
    "\n",
    "Student ID: `400100616`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "Nowadays machine learning algorithms and models are used to solve too many types of problems. One of the most important problems is sequence modeling. Sequence modeling is the ability of an algorithm to model, interpret, make predictions about, or generate sequence data like audio, text, etc.\n",
    "\n",
    "One of the most used algorithms to solve sequence modeling problems is the Recurrent Neural Network which is a specialized form of the classical Artificial Neural Network (Multi-Layer Perceptron). Below is a picture of this network architecture:\n",
    "\n",
    "<img src=\"RNN.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "In sequences there is a common feature, that each value has some dependency on its previous members. A simple example is arithmetic sequence: $a_i = i * d + a_0, 0 \\leq i$, where $a_n$ is dependent on $a_{n-1}$. So based on this nature of the sequences, there is a loop inside the RNN cell. This helps the RNN cell to remember some information about the previous activation values. \n",
    "\n",
    "In general, RNN is used to model sequences because of the following advantages:\n",
    "+ handles variable-length dependencies.\n",
    "+ Tracks dependencies.\n",
    "+ Maintains information about the order.\n",
    "+ Shares parameters across the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you are going to do in this notebook\n",
    "\n",
    "+ `Step1.` You have to design a simple neural network that has only 4 neurons, inspired by the RNNs architecture, and use it to predict the next number of an arithmetic sequence.\n",
    "\n",
    "+ `Step2.` You have to increase the number of the neurons to 8. Then you have create a complex dataset of arithmetic sequences, that in each with a number of `d` called $1 \\leq n$.\n",
    "\n",
    "+ `Step3.` In the previous sections, you will find out why just using the basic architecture of RNNs does not work for complex data. So, for this step, you have to implement a Gated Recurrent Unit (GRU) from scrach to train and test it over the MNIST dataset and compare it with PyTorch GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# START TO LEARN ABOUT RNNs (30 points)\n",
    "Step 1. First, we start with simple data, like arithmetic sequence: $a_i = i * d + a_0, 0 \\leq i$.\n",
    "\n",
    "You have to design a simple neural network (only 4 neurons) inspired from the RNNs to predict the next number in the sequence.\n",
    "\n",
    "`Note`: First, try to solve this problem theoretically (find the final values for your network's weights) and then implement it.\n",
    "\n",
    "`your answer`\n",
    "\n",
    "#### **Theoretical Solution**\n",
    "To predict the next number in an arithmetic sequence \\( a_i = i \\cdot d + a_0 \\), we design a simple RNN-inspired neural network with 4 neurons. The model works as follows:\n",
    "\n",
    "1. **Sequence Dependency**: \n",
    "   - Each term in the sequence depends on the previous term and the difference \\( d \\).\n",
    "   - The RNN uses a hidden state to store the sequence's current state and updates it to predict the next term.\n",
    "\n",
    "2. **Network Architecture**:\n",
    "   - The network consists of a single hidden layer with 4 neurons and an output layer.\n",
    "   - The hidden state is updated based on:\n",
    "     \\[\n",
    "     h_{t+1} = W_{\\text{rec}} \\cdot h_t + W_{\\text{in}} \\cdot x_t\n",
    "     \\]\n",
    "   - The output \\( y_{t+1} \\) is computed directly from \\( h_{t+1} \\), where \\( x_t \\) represents the difference \\( d \\).\n",
    "\n",
    "3. **Weight Design**:\n",
    "   - Recurrent weight (\\( W_{\\text{rec}} \\)) is set to 1, allowing the network to propagate the current state.\n",
    "   - Input weight (\\( W_{\\text{in}} \\)) is set to \\( d \\), scaling the difference for predicting the next term.\n",
    "\n",
    "4. **Prediction Rule**:\n",
    "   - For a given input \\( a_0 \\), the network iteratively predicts the next sequence terms based on the arithmetic sequence rule:\n",
    "     \\[\n",
    "     a_{i+1} = a_i + d\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implementation**\n",
    "The RNN is implemented in Python using the following steps:\n",
    "1. Initialize the weights and hidden state.\n",
    "2. Define a loop to calculate the next hidden state and output for each timestep.\n",
    "3. Generate the sequence and compare it with the theoretical sequence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:20:37.322807Z",
     "start_time": "2025-01-02T00:20:33.929584Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:20:37.326060Z",
     "start_time": "2025-01-02T00:20:37.323526Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def CustomDataset(max_length=20, size=10000): \n",
    "    \"\"\"\n",
    "    Generate your dataset with the following details:\n",
    "        - -100 <= a0 <= 100\n",
    "        - -20 <= d <= 20\n",
    "        - 1 <= len(sequence) <= max_length\n",
    "\n",
    "    Parameters:\n",
    "        max_length (int): Maximum length of any sequence. Default is 20.\n",
    "        size (int): Number of sequences to generate. Default is 10000.\n",
    "\n",
    "    Returns:\n",
    "        dataset (list): A list of tuples in the format:\n",
    "                        (length, sequence, next_number)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        # Randomly choose the initial term a0 and common difference d\n",
    "        a0 = np.random.randint(-100, 101)     # Range: [-100, 100]\n",
    "        d = np.random.randint(-20, 21)        # Range: [-20, 20]\n",
    "\n",
    "        # Randomly choose the length of the sequence (at least 1, up to max_length)\n",
    "        length = np.random.randint(1, max_length + 1)\n",
    "\n",
    "        # Generate the arithmetic sequence\n",
    "        sequence = [a0 + i*d for i in range(length)]\n",
    "\n",
    "        # The \"next number\" is simply the next term in the arithmetic progression\n",
    "        next_number = a0 + length*d\n",
    "\n",
    "        # Append the tuple (length, sequence, next_number) to the dataset\n",
    "        dataset.append((length, sequence, next_number))\n",
    "    \n",
    "    return dataset\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now create the model:\n",
    "Based on the this task design your model. \n",
    "+ Note that, your model should inspire from the behavior of RNNs.\n",
    "+ Solve this problem theoretically. Then you will find out you only need 4 neurons."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:20:37.329139Z",
     "start_time": "2025-01-02T00:20:37.326607Z"
    }
   },
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = 4\n",
    "        self.rnn_cell = nn.RNNCell(1, self.hidden_size)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h = torch.zeros(batch_size, self.hidden_size)\n",
    "        for i in range(x.size(1)):\n",
    "            h = self.rnn_cell(x[:, i].unsqueeze(1), h)\n",
    "        out = self.fc1(h)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Now you have to train your model. \n",
    "\n",
    "Note, your architecture must be based on RNNs. This means that you have to set a hidden state called `h` and use it to create a loop for your model like RNNs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:21:26.539294Z",
     "start_time": "2025-01-02T00:20:37.330485Z"
    }
   },
   "source": [
    "# Create your dataset\n",
    "dataset = CustomDataset()  # Each element: (length, sequence, next_number)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Model()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for length, seq, next_val in dataset:\n",
    "        # 1) Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2) Convert 'seq' into a torch tensor of shape (1, length)\n",
    "        #    We use unsqueeze(0) so that the batch dimension is 1.\n",
    "        #    Each element in seq is a scalar, hence dtype=torch.float32.\n",
    "        x_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # 3) Convert 'next_val' into a torch tensor of shape (1,)\n",
    "        y_tensor = torch.tensor([next_val], dtype=torch.float32)\n",
    "\n",
    "        # 4) Forward pass\n",
    "        output = model(x_tensor)  # Now x_tensor is a proper tensor\n",
    "        \n",
    "        # 5) Compute the loss\n",
    "        #    output has shape (1, 1); after squeezing, it will be (1,)\n",
    "        #    y_tensor has shape (1,). So the shapes match for MSELoss.\n",
    "        loss = criterion(output.squeeze(), y_tensor)\n",
    "        \n",
    "        # 6) Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 7) Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 8) Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 7962.671392\n",
      "Epoch [2/10], Loss: 607.973550\n",
      "Epoch [3/10], Loss: 379.113353\n",
      "Epoch [4/10], Loss: 161.901138\n",
      "Epoch [5/10], Loss: 132.625925\n",
      "Epoch [6/10], Loss: 112.787298\n",
      "Epoch [7/10], Loss: 74.178968\n",
      "Epoch [8/10], Loss: 68.926701\n",
      "Epoch [9/10], Loss: 64.335050\n",
      "Epoch [10/10], Loss: 50.454033\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Evaluation\n",
    "Print the weights of the model.\n",
    "\n",
    "You can see, model's weight are exactly same as what you have found in theoretical way."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:21:26.545503Z",
     "start_time": "2025-01-02T00:21:26.540374Z"
    }
   },
   "source": [
    "# TODO\n",
    "# Weights Evaluation\n",
    "# Print the weights of the model.\n",
    "\n",
    "# Access and print weights of the hidden and output layers\n",
    "print(\"Model Weights:\")\n",
    "weights = {name: param.data for name, param in model.named_parameters()}\n",
    "print(weights)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights:\n",
      "{'rnn_cell.weight_ih': tensor([[ 0.0030],\n",
      "        [-0.0078],\n",
      "        [ 0.0140],\n",
      "        [-0.0138]]), 'rnn_cell.weight_hh': tensor([[ 0.8067,  1.0791, -0.8036,  0.6509],\n",
      "        [-0.2776, -0.2955,  0.4486,  0.1493],\n",
      "        [ 1.3597,  1.6352, -0.4425,  1.8459],\n",
      "        [ 0.6763,  0.0168,  0.1593, -0.5365]]), 'rnn_cell.bias_ih': tensor([-0.2884, -0.5720,  0.0073,  1.7616]), 'rnn_cell.bias_hh': tensor([-0.2884, -0.5720,  0.0073,  1.7616]), 'fc1.weight': tensor([[ 7.1821,  5.2134, -6.0177,  8.4018],\n",
      "        [ 6.9168,  6.9473, -6.2598,  8.1238]]), 'fc1.bias': tensor([0.1095, 0.1149]), 'fc2.weight': tensor([[-8.0885, -7.9394]]), 'fc2.bias': tensor([-0.0989])}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer\n",
    "\n",
    "The hidden state in this model plays a crucial role in maintaining information about previous computations across sequence steps. By incorporating a hidden state, the model mimics the behavior of Recurrent Neural Networks (RNNs), which are designed to handle sequential data effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **How the Model Uses the Hidden State**\n",
    "In each step of the sequence, the input data (e.g., individual sequence elements) is processed through the recurrent layer. The hidden state is updated dynamically at every step using:\n",
    "- The current input, and\n",
    "- The previous hidden state.\n",
    "\n",
    "This iterative update allows the model to:\n",
    "1. Retain memory of past inputs.\n",
    "2. Dynamically adjust the representation of the sequence as it processes more data.\n",
    "\n",
    "The final hidden state serves as a summarized representation of the sequence, encapsulating all the relevant information needed for predicting the next term in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why the Hidden State is Helpful**\n",
    "\n",
    "1. **Capturing Long-Term Dependencies**:  \n",
    "   The hidden state allows the model to remember dependencies between distant elements in the sequence, which is essential for modeling patterns in sequential data.\n",
    "\n",
    "2. **Efficient Parameter Usage**:  \n",
    "   The hidden state mechanism reuses the same weights across all timesteps. This sharing of parameters reduces the model’s overall complexity and improves its generalization ability.\n",
    "\n",
    "3. **Preserving Order and Context**:  \n",
    "   For tasks where the order of elements is critical (e.g., time series, text), the hidden state preserves the sequential context by continuously updating its representation based on the current input and past information.\n",
    "\n",
    "4. **Summarizing Information**:  \n",
    "   By the end of the sequence, the hidden state serves as a compact summary of all preceding elements, enabling the model to use this summary for the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "Overall, the hidden state enables the model to learn and leverage temporal patterns, making it a powerful tool for sequence modeling tasks like arithmetic sequences, language processing, and time-series forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of long-term dependencies (40 points)\n",
    "Step 2. Now, we are going to do some analysis. In the previous, both the dataset and model were simple. Now, we want to make it a little complex. \n",
    "\n",
    "+ Assume the lengths for all sequences are equal.\n",
    "+ Consider each sequence has `n` number of `d`:\n",
    "    - e.g, $a_0, a_0 + d_0, a_1 + d_1, a_2 + d_2, ...., a_{n-1} + d_{n-1}, a_{n} + d_0, a_{n+1} + d_1, ...$\n",
    "+ Change your model: just increase the number of neurons to 8. Also, because your input's dimension has changed, you have to increase the hidden state `h` dimension.\n",
    "+ Create a loop over all possible $2 \\leq n \\leq 10$ and generate a dataset for each one. Then train a new model for each dataset.\n",
    "+ At the end, show a `TotalLoss - numberOfd` plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:21:26.549316Z",
     "start_time": "2025-01-02T00:21:26.546110Z"
    }
   },
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Design a model with:\n",
    "    - 8 hidden neurons in the RNN cell\n",
    "    - Hidden state dimension adjusted to match new input dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=8, output_size=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the RNN Cell with updated hidden size\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layers for output transformation\n",
    "        self.fc1 = nn.Linear(hidden_size, 8)\n",
    "        self.fc2 = nn.Linear(8, output_size)\n",
    "\n",
    "        # Initialize weights for stability\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, sequence_length, input_size)\n",
    "           Each sequence element is a single scalar => input_size=1\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "\n",
    "        # Initialize the hidden state to zeros\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        # Process each element of the sequence\n",
    "        for t in range(seq_length):\n",
    "            input_t = x[:, t].unsqueeze(1)  # Shape: (batch_size, input_size)\n",
    "            h = self.rnn_cell(input_t, h)\n",
    "\n",
    "        # Pass the final hidden state through fully connected layers\n",
    "        out = self.fc1(h)      # Shape: (batch_size, 16)\n",
    "        out = self.fc2(out)    # Shape: (batch_size, output_size)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:21:26.552202Z",
     "start_time": "2025-01-02T00:21:26.549830Z"
    }
   },
   "source": [
    "def CustomDataset(n, length=20, size=10000):\n",
    "    \"\"\"\n",
    "    Generate your dataset with the following details:\n",
    "        -100 <= a0 <= 100\n",
    "        -20 <= d_i <= 20 (for each of the n d values)\n",
    "        \n",
    "    Parameters:\n",
    "        - n: number of d values in the sequences\n",
    "        - length: length of all sequences (default is 20)\n",
    "        - size: number of sequences to generate (default is 10000)\n",
    "\n",
    "    Returns:\n",
    "        - dataset: A list of tuples, each containing:\n",
    "                   (length, sequence, next_number in the sequence)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        # Random initial term\n",
    "        a0 = np.random.randint(-100, 101)  # Range: [-100, 100]\n",
    "\n",
    "        # Generate n different d values\n",
    "        d_values = [np.random.randint(-20, 21) for _ in range(n)]  # Range: [-20, 20]\n",
    "\n",
    "        # Create the sequence using cyclic d values\n",
    "        sequence = [a0]\n",
    "        for i in range(1, length):\n",
    "            sequence.append(sequence[-1] + d_values[i % n])\n",
    "\n",
    "        # Determine the next number in the sequence\n",
    "        next_number = sequence[-1] + d_values[length % n]\n",
    "\n",
    "        # Add to dataset\n",
    "        dataset.append((length, sequence, next_number))\n",
    "    \n",
    "    return dataset\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:31:55.827590Z",
     "start_time": "2025-01-02T00:21:26.552755Z"
    }
   },
   "source": [
    "losses = []\n",
    "epochs = 10\n",
    "\n",
    "for n in range(2, 10):\n",
    "    total_loss = 0\n",
    "    # Create your dataset, using the CustomDataset function\n",
    "    dataset = CustomDataset(n=n, length=20, size=10000)\n",
    "\n",
    "    # Initialize model, loss function (use MSE Loss function), and optimizer (use adam optimizer)\n",
    "    model = Model(input_size=1, hidden_size=8, output_size=1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Optional: Use a scheduler for learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for _, seq, next_val in dataset:\n",
    "            # Convert sequence and next value to tensors\n",
    "            x_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # Shape: (1, sequence_length, 1)\n",
    "            y_tensor = torch.tensor([next_val], dtype=torch.float32)        # Shape: (1,)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x_tensor)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output.squeeze(), y_tensor)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for the epoch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        total_loss = epoch_loss / len(dataset)\n",
    "\n",
    "    # Append the total loss for this `n`\n",
    "    losses.append(total_loss)\n",
    "    print(f\"n = {n}, Total Loss: {total_loss:.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2, Total Loss: 21.016307\n",
      "n = 3, Total Loss: 67.127602\n",
      "n = 4, Total Loss: 169.712165\n",
      "n = 5, Total Loss: 166.012990\n",
      "n = 6, Total Loss: 165.957464\n",
      "n = 7, Total Loss: 164.597701\n",
      "n = 8, Total Loss: 167.213431\n",
      "n = 9, Total Loss: 165.499059\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:31:56.151864Z",
     "start_time": "2025-01-02T00:31:55.829895Z"
    }
   },
   "source": [
    "# plot your amount of loss and number of d in datasets\n",
    "# TODO\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 10), losses, marker='o', label=\"Total Loss\")\n",
    "plt.title(\"Total Loss vs Number of d Values (n)\")\n",
    "plt.xlabel(\"Number of d Values (n)\")\n",
    "plt.ylabel(\"Total Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your opinion\n",
    "\n",
    "+ You can see, as the number of d in the sequences increases the amount of the total loss increases too. Why?\n",
    "    - `your answer`\n",
    "\n",
    "+ Can you mathematically explain your opinion? (help: There are some gradient issues!)\n",
    "    - `your answer`\n",
    "\n",
    "+ Can you explain the problem of Long-term dependencies in RNNs?\n",
    "    - `your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRUs) (30 points)\n",
    "\n",
    "Step3. In the previous section, you saw, simple recurrent models cannot remember information from the past. This is a big problem because in real-world applications sequences have a lot of dependencies on their past time steps and we have to find a way to remember the information from the past. To address this problem, researchers have designed other architecture that can selectively remember or forget information over time. Like: `Long-Short Term Memory (LSTM)` and `Gated Recurrent Unit (GRU)`. The key building block behind these architectures is `gate`.\n",
    "\n",
    "#### GATE:\n",
    "These networks, use gates to track information throughout many time steps:\n",
    "+ Add information, when the information is needed.\n",
    "+ Remover information, When information is not important\n",
    "\n",
    "<img src=\"GATE.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### Long-Short Term Memory:\n",
    "LSTM is an improved version of RNNs. In a classical recurrent model, there is a single hidden state `h` which is used to help the model to remember some information over time. But this is not enough to learn long-term dependencies. Using gates, LSTM is capable of addressing the problem of long-term dependencies. \n",
    "\n",
    "LSTM cell:\n",
    "\n",
    "<img src=\"LSTM.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "GATES:\n",
    "+ <font color=green size=3> Forget gate:</font> To forget irrelevant information.\n",
    "+ <font color=red size=3> Store gate:</font> Decide what part of new is relevant.\n",
    "+ <font color=orange size=3> Update:</font> update cell state values.\n",
    "+ <font color=blue size=3> Output gate:</font> Controls what information is sent to the next time step.\n",
    " \n",
    "This architecture addresses the gradient issues you have described in the previous section. (How?)\n",
    "\n",
    "`write your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated Recurrent Unit (GRU):\n",
    "Like LSTM, GRU is designed to model sequential data by allowing information to be selectively remembered or forgotten over time.\n",
    "\n",
    "GRU cell:\n",
    "\n",
    "<img src=\"GRU.jpg\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATES and other parts:\n",
    "+ <font color=green size=3> Reset gate:</font> How much of previous hidden state to forget.\n",
    "    - $r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n",
    "+ <font color=orange size=3> Update gate:</font> How much of the candidate activation vector to incorporate into the new hidden state.\n",
    "    - $z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n",
    "    \n",
    "+ <font color=red size=3> Candidate Activation vector:</font> Computed using the current input x and modified version of the previous hidden state that is `reset` by the reset gate.\n",
    "    - $\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t])$\n",
    "\n",
    "+ <font color=blue size=3> Hidden state</font>\n",
    "    - $h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n",
    "\n",
    "Note: the $W_r$, $W_z$, $W_h$ are the weights associated with the GRU cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Implementation\n",
    "Now, implement a Gated Recurrent Unit from scratch."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:31:56.175818Z",
     "start_time": "2025-01-02T00:31:56.172453Z"
    }
   },
   "source": [
    "class GRU_cell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_calsses):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_calsses\n",
    "\n",
    "        self.reset_gate = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        self.update_gate = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        self.candidate = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.output = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        \n",
    "        # TODO\n",
    "        # define activation functions\n",
    "        \n",
    "    def forward(self, x, h, end = False):\n",
    "        # TODO \n",
    "        # (reset gate, update gate, candidate vector, hidden state, output)\n",
    "        # feel free to change the input parameter\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "MNIST database is a collection of handwritten digits. You are going to use this database to train and test your GRU cell.\n",
    "\n",
    "First, to download this database run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T00:31:56.178321Z",
     "start_time": "2025-01-02T00:31:56.176438Z"
    }
   },
   "source": [
    "database = MNIST(root = 'data/', download = True)\n",
    "# TODO\n",
    "# get train and test data - use transforms.ToTensor() to convert images to tensor\n",
    "train_data = \n",
    "test_data = "
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2723446608.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[11], line 4\u001B[0;36m\u001B[0m\n\u001B[0;31m    train_data =\u001B[0m\n\u001B[0m                 ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the image and label of one instance from the train data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO\n",
    "# show one image tensor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a generate custom dataset function\n",
    "+ For each data in the database convert, use one-hot encoding from pytorch to encode your label to a vector. This is necessary for computing the amount of loss.\n",
    "+ It is common to use batches of data to train our model simultaneously on a batch. The length of a batch at most is `BATCH_SIZE`.\n",
    "    - Note: you can also train your model on each data (one by one) but the training part will take too much time. \n",
    "+ Return batches of data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "HIDDEN_DIM = 15\n",
    "INPUT_DIM = 28\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def CustomDataset(data, batch_size = BATCH_SIZE):\n",
    "    \"\"\" \n",
    "        Generate batches of data. Use one-hot encoding to encode your labels according to number of classes.\n",
    "        \n",
    "        Parameters:\n",
    "            - data: MNIST database\n",
    "            - batch_size = BATCH_SIZE\n",
    "\n",
    "        Returns:\n",
    "            - batches = A list of [(image_tensor, encoded label)]. Each batch's length is at most BATCH_SIZE\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    for image_tensor, label in data:\n",
    "        encoded_label = # use one-hot encoding\n",
    "        encoded_label = torch.tensor(encoded_label, dtype=torch.float32).unsqueeze(dim = 0)\n",
    "        # TODO\n",
    "\n",
    "    return batches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "write your training loop.\n",
    "+ For loss function: use `CrossEntropyLoss` function\n",
    "+ Use adam optimizer\n",
    "+ To optimize your training process, you can use `optim.lr_schduler`. (Why this will improve the training process?)\n",
    "    - `your answer`\n",
    "+ Note: Also, in each time step give one row of image tensor to the model because your model is sequential (you also can use each column)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO\n",
    "# create batches\n",
    "batches = CustomDataset(train_data)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = GRU_cell(INPUT_DIM, HIDDEN_DIM, NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# use scheduler to set learning rate\n",
    "\n",
    "# training loop\n",
    "# TODO\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in batches:\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Test data\n",
    "Use the test data to evaluate your model.\n",
    "+ Print the accuracy of your model (should be greater than $94\\%$).\n",
    "+ Show some predictions with actual value from the test data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # compute the accuracy of your model\n",
    "    # TODO\n",
    "    count = 0\n",
    "    dataset = CustomDataset(test_data)\n",
    "\n",
    "    for x, y in dataset:\n",
    "        # TODO\n",
    "        pass\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# show images of 50 predictions with actual values "
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
